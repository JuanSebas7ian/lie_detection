{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mNo se pudo iniciar el Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'Python 3.9.13' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importar librer√≠as\n",
    "import re, os\n",
    "import math\n",
    "import nltk\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from helper_prabowo_ml import clean_html, remove_links, non_ascii, lower, email_address, removeStopWords, punct, remove_\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from transformers import AutoTokenizer, TFBertModel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mNo se pudo iniciar el Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'Python 3.9.13' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def remove_nan_rows(dataframe, columns=['messages', 'sender_labels', 'receiver_labels']):\n",
    "    \"\"\"\n",
    "    Remove rows with NaN values in specified columns.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframe: pd.DataFrame\n",
    "        The DataFrame to process.\n",
    "    - columns: list\n",
    "        List of columns to check for NaN values.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame\n",
    "        DataFrame with rows containing NaN values removed.\n",
    "    \"\"\"\n",
    "    return dataframe.dropna(subset=columns)\n",
    "\n",
    "\n",
    "def remove_tags(string, remove_special_chars=False, remove_stopwords=False, remove_newlines=False):\n",
    "    result = re.sub(r'<.*?>', '', string)  # Remove HTML tags\n",
    "    result = re.sub('@[\\w]+', '', result)  # Remove Twitter usernames\n",
    "    result = re.sub('#[\\w]+', '', result)  # Remove hashtags\n",
    "    result = re.sub(\"\\d+\", \" \", result)  # Remove numbers\n",
    "    result = re.sub(r'http\\S+', '', result)  # Remove URLs\n",
    "\n",
    "    if remove_special_chars:\n",
    "        result = re.sub(r'[^\\w\\s]', ' ', result)  # Remove non-alphanumeric characters\n",
    "\n",
    "    if remove_newlines:\n",
    "        result = re.sub(r'\\n\\n', ' ', result)  # Remove newline characters\n",
    "        result = ' '.join(result.split())  # Split and join to remove extra spaces\n",
    "\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        result = ' '.join([w for w in result.split() if w.lower() not in stop_words])\n",
    "\n",
    "    # Remove words of length 1\n",
    "    result = ' '.join([word for word in result.split() if len(word) > 1])\n",
    "\n",
    "    result = result.lower()\n",
    "    return result\n",
    "\n",
    "def compute_class_weight(train_y):\n",
    "    \"\"\"\n",
    "    Compute class weight given imbalanced training data\n",
    "    Usually used in the neural network model to augment the loss function (weighted loss function)\n",
    "    Favouring/giving more weights to the rare classes.\n",
    "    \"\"\"\n",
    "    import sklearn.utils.class_weight as scikit_class_weight\n",
    "\n",
    "    class_list = list(set(train_y))\n",
    "    class_weight_value = scikit_class_weight.compute_class_weight(class_weight ='balanced', classes = class_list, y = train_y)\n",
    "    class_weight = dict()\n",
    "\n",
    "    # Initialize all classes in the dictionary with weight 1\n",
    "    curr_max = int(np.max(class_list))\n",
    "    for i in range(curr_max):\n",
    "        class_weight[i] = 1\n",
    "\n",
    "    # Build the dictionary using the weight obtained the scikit function\n",
    "    for i in range(len(class_list)):\n",
    "        class_weight[class_list[i]] = class_weight_value[i]\n",
    "\n",
    "    return class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "messages = []\n",
    "with open('NLP_Diplomacy/train.jsonl', 'r') as archivo:\n",
    "    for line in archivo:\n",
    "        # Load each line as a JSON object\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "            data_list.append(data)\n",
    "            messages.extend(data['messages'])\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "            \n",
    "            \n",
    "validation_list = []\n",
    "messages = []\n",
    "with open('NLP_Diplomacy/validation.jsonl', 'r') as archivo:\n",
    "    for line in archivo:\n",
    "        # Load each line as a JSON object\n",
    "        try:\n",
    "            validation = json.loads(line)\n",
    "            validation_list.append(validation)\n",
    "            messages.extend(validation['messages'])\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "            \n",
    "test_list = []\n",
    "messages = []\n",
    "with open('NLP_Diplomacy/test.jsonl', 'r') as archivo:\n",
    "    for line in archivo:\n",
    "        # Load each line as a JSON object\n",
    "        try:\n",
    "            test = json.loads(line)\n",
    "            test_list.append(validation)\n",
    "            messages.extend(validation['messages'])\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "            \n",
    "df = pd.DataFrame(data_list)\n",
    "df_val = pd.DataFrame(validation_list)\n",
    "df_test = pd.DataFrame(test_list)\n",
    "            \n",
    "df_explode=df.explode(['messages',\n",
    " 'sender_labels',\n",
    " 'receiver_labels',\n",
    " 'speakers',\n",
    " 'receivers',\n",
    " 'absolute_message_index',\n",
    " 'relative_message_index',\n",
    " 'seasons',\n",
    " 'years',\n",
    " 'game_score',\n",
    " 'game_score_delta'\n",
    " ], ignore_index=True)\n",
    "\n",
    "df_val=df_val.explode(['messages',\n",
    " 'sender_labels',\n",
    " 'receiver_labels',\n",
    " 'speakers',\n",
    " 'receivers',\n",
    " 'absolute_message_index',\n",
    " 'relative_message_index',\n",
    " 'seasons',\n",
    " 'years',\n",
    " 'game_score',\n",
    " 'game_score_delta'\n",
    " ], ignore_index=True)\n",
    "\n",
    "df_test=df_test.explode(['messages',\n",
    " 'sender_labels',\n",
    " 'receiver_labels',\n",
    " 'speakers',\n",
    " 'receivers',\n",
    " 'absolute_message_index',\n",
    " 'relative_message_index',\n",
    " 'seasons',\n",
    " 'years',\n",
    " 'game_score',\n",
    " 'game_score_delta'\n",
    " ], ignore_index=True)\n",
    "\n",
    "# Assuming df_explode is your DataFrame\n",
    "df_explode_cleaned = remove_nan_rows(df_explode)\n",
    "\n",
    "df_val_cleaned = remove_nan_rows(df_val)\n",
    "\n",
    "df_test_cleaned = remove_nan_rows(df_test)\n",
    "\n",
    "# Assuming df_explode_cleaned is your DataFrame\n",
    "df_explode_cleaned['messages'] = df_explode_cleaned['messages'].astype(str)\n",
    "df_explode_cleaned.loc[:, 'messages_clean'] = df_explode_cleaned['messages'].apply(lambda cw: remove_tags(cw, remove_special_chars=True, remove_stopwords=True, remove_newlines=True))\n",
    "\n",
    "# Assuming df_explode_cleaned is your DataFrame\n",
    "df_val_cleaned['messages'] = df_val_cleaned['messages'].astype(str)\n",
    "df_val_cleaned.loc[:, 'messages_clean'] = df_val_cleaned['messages'].apply(lambda cw: remove_tags(cw, remove_special_chars=True, remove_stopwords=True, remove_newlines=True))\n",
    "\n",
    "# Assuming df_explode_cleaned is your DataFrame\n",
    "df_test_cleaned['messages'] = df_test_cleaned['messages'].astype(str)\n",
    "df_test_cleaned.loc[:, 'messages_clean'] = df_test_cleaned['messages'].apply(lambda cw: remove_tags(cw, remove_special_chars=True, remove_stopwords=True, remove_newlines=True))\n",
    "\n",
    "\n",
    "df_explode_cleaned = remove_nan_rows(df_explode_cleaned)\n",
    "df_val_cleaned = remove_nan_rows(df_val_cleaned)\n",
    "df_test_cleaned = remove_nan_rows(df_test_cleaned)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
